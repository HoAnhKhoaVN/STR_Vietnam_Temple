@article{00c8fb5e9be84801024842fb6be6814b361a4483,
title = {SwapText: Image Based Texts Transfer in Scenes},
year = {2020},
url = {https://www.semanticscholar.org/paper/00c8fb5e9be84801024842fb6be6814b361a4483},
abstract = {Swapping text in scene images while preserving original fonts, colors, sizes and background textures is a challenging task due to the complex interplay between different factors. In this work, we present SwapText, a three-stage framework to transfer texts across scene images. First, a novel text swapping network is proposed to replace text labels only in the foreground image. Second, a background completion network is learned to reconstruct background images. Finally, the generated foreground image and background image are used to generate the word image by the fusion network. Using the proposing framework, we can manipulate the texts of the input images even with severe geometric distortion. Qualitative and quantitative results are presented on several scene text datasets, including regular and irregular text datasets. We conducted extensive experiments to prove the usefulness of our method such as image based text translation, text image synthesis.},
author = {Qiangpeng Yang and Hongsheng Jin and Jun Huang and Wei Lin},
journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
volume = {null},
pages = {14688-14697},
doi = {10.1109/cvpr42600.2020.01471},
arxivid = {2003.08152},
}

@article{b8fe2b02776208670906c89f8b8d361074fc87d5,
title = {Exploring Stroke-Level Modifications for Scene Text Editing},
year = {2022},
url = {https://www.semanticscholar.org/paper/b8fe2b02776208670906c89f8b8d361074fc87d5},
abstract = {Scene text editing (STE) aims to replace text with the de- sired one while preserving background and styles of the original text. However, due to the complicated background tex- tures and various text styles, existing methods fall short in generating clear and legible edited text images. In this study, we attribute the poor editing performance to two problems: 1) Implicit decoupling structure. Previous methods of edit- ing the whole image have to learn different translation rules of background and text regions simultaneously. 2) Domain gap. Due to the lack of edited real scene text images, the network can only be well trained on synthetic pairs and performs poorly on real-world images. To handle the above problems, we propose a novel network by MOdifying Scene Text image at strokE Level (MOSTEL). Firstly, we generate stroke guidance maps to explicitly indicate regions to be edited. Different from the implicit one by directly modifying all the pixels at image level, such explicit instructions ﬁlter out the distrac- tions from background and guide the network to focus on editing rules of text regions. Secondly, we propose a Semi- supervised Hybrid Learning to train the network with both labeled synthetic images and unpaired real scene text images. Thus, the STE model is adapted to real-world datasets distributions. Moreover, two new datasets (Tamper-Syn2k and Tamper-Scene) are proposed to ﬁll the blank of public evaluation datasets. Extensive experiments demonstrate that our MOSTEL outperforms previous methods both qualitatively and quantitatively. Datasets and code will be available at https://github.com/qqqyd/MOSTEL.},
author = {Yadong Qu and Qingfeng Tan and Hongtao Xie and Jianjun Xu and Yuxin Wang and Yongdong Zhang},
journal = {ArXiv},
volume = {abs/2212.01982},
pages = {null},
doi = {10.48550/arXiv.2212.01982},
arxivid = {2212.01982},
}

@article{0ff76d661822bed3e7b378cb8efb36c101359ede,
title = {Detecting and Removing Text in the Wild},
year = {2021},
url = {https://www.semanticscholar.org/paper/0ff76d661822bed3e7b378cb8efb36c101359ede},
abstract = {Scene text removal is a challenging task that aims to erase wild text regions that include text strokes and their ambiguous boundaries, such as embossing, shade, or flare. The challenging issues raised in the wild are not completely addressed by the existing methods. To address these issues, we propose a new loss function for blending two tasks in a new network structure that depicts wild text regions in a soft mask and selectively inpaints them into a sensible background. The proposed loss function aids the learning of two seemingly separate tasks in a synergistic way via the soft mask to achieve remarkable performance in scene text removal. We validate our method through qualitative and quantitative comparisons, and region-wise analysis, showing that our method outperforms existing methods.},
author = {Junho Cho and Sangdoo Yun and Dongyoon Han and Byeongho Heo and J. Choi},
journal = {IEEE Access},
volume = {9},
pages = {123313-123323},
doi = {10.1109/ACCESS.2021.3110293},
}

@article{07452630b33d79d051b1c59599cccc13d58879ec,
title = {TextStyleBrush: Transfer of Text Aesthetics from a Single Example},
year = {2021},
url = {https://www.semanticscholar.org/paper/07452630b33d79d051b1c59599cccc13d58879ec},
abstract = {We present a novel approach for disentangling the content of a text image from all aspects of its appearance. The appearance representation we derive can then be applied to new content, for one-shot transfer of the source style to new content. We learn this disentanglement in a self-supervised manner. Our method processes entire word boxes, without requiring segmentation of text from background, per-character processing, or making assumptions on string lengths. We show results in different text domains which were previously handled by specialized methods, e.g., scene text, handwritten text. To these ends, we make a number of technical contributions: (1) We disentangle the style and content of a textual image into a non-parametric, fixed-dimensional vector. (2) We propose a novel approach inspired by StyleGAN but conditioned over the example style at different resolution and content. (3) We present novel self-supervised training criteria which preserve both source style and target content using a pre-trained font classifier and text recognizer. Finally, (4) we also introduce Imgur5K, a new challenging dataset for handwritten word images. We offer numerous qualitative photo-realistic results of our method. We further show that our method surpasses previous work in quantitative tests on scene text and handwriting datasets, as well as in a user study.},
author = {Praveen Krishnan and Rama Kovvuri and G. Pang and B. Vassilev and Tal Hassner},
journal = {ArXiv},
volume = {abs/2106.08385},
pages = {null},
doi = {10.1109/tpami.2023.3239736},
arxivid = {2106.08385},
}

@article{8faddb00bb5b99013ec359475ba7787ff7e05229,
title = {Multi-content GAN for Few-Shot Font Style Transfer},
year = {2017},
url = {https://www.semanticscholar.org/paper/8faddb00bb5b99013ec359475ba7787ff7e05229},
abstract = {In this work, we focus on the challenge of taking partial observations of highly-stylized text and generalizing the observations to generate unobserved glyphs in the ornamented typeface. To generate a set of multi-content images following a consistent style from very few examples, we propose an end-to-end stacked conditional GAN model considering content along channels and style along network layers. Our proposed network transfers the style of given glyphs to the contents of unseen ones, capturing highly stylized fonts found in the real-world such as those on movie posters or infographics. We seek to transfer both the typographic stylization (ex. serifs and ears) as well as the textual stylization (ex. color gradients and effects.) We base our experiments on our collected data set including 10,000 fonts with different styles and demonstrate effective generalization from a very small number of observed glyphs.},
author = {S. Azadi and Matthew Fisher and Vladimir G. Kim and Zhaowen Wang and Eli Shechtman and Trevor Darrell},
journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
volume = {null},
pages = {7564-7573},
doi = {10.1109/CVPR.2018.00789},
arxivid = {1712.00516},
}

@article{4e46620ebed2c6e4f3335efe5e9338820ead1ee7,
title = {RewriteNet: Reliable Scene Text Editing with Implicit Decomposition of Text Contents and Styles},
year = {2021},
url = {https://www.semanticscholar.org/paper/4e46620ebed2c6e4f3335efe5e9338820ead1ee7},
abstract = {Scene text editing (STE), which converts a text in a scene image into the desired text while preserving an original style, is a challenging task due to a complex intervention between text and style. In this paper, we propose a novel STE model, referred to as RewriteNet, that decomposes text images into content and style features and re-writes a text in the original image. Speciﬁcally, RewriteNet implicitly distinguishes the content from the style by intro-ducing scene text recognition. Additionally, independent of the exact supervisions with synthetic examples, we propose a self-supervised training scheme for unlabeled real-world images, which bridges the domain gap between synthetic and real data. Our experiments present that RewriteNet achieves better generation performances than other comparisons. Further analysis proves the feature decomposition of RewriteNet and demonstrates the reliability and robustness through diverse experiments. Our implementation is publicly available at https://github.com/clova ai/rewritenet},
author = {Junyeop Lee and Yoonsik Kim and Seonghyeon Kim and Moonbin Yim and Seung Shin and Gayoung Lee and Sungrae Park},
arxivid = {2107.11041},
}

@article{d903b032b2ce87feb00735b1aca40e3535c63ba7,
title = {UnrealText: Synthesizing Realistic Scene Text Images from the Unreal World},
year = {2020},
url = {https://www.semanticscholar.org/paper/d903b032b2ce87feb00735b1aca40e3535c63ba7},
abstract = {Synthetic data has been a critical tool for training scene text detection and recognition models. On the one hand, synthetic word images have proven to be a successful substitute for real images in training scene text recognizers. On the other hand, however, scene text detectors still heavily rely on a large amount of manually annotated real-world images, which are expensive. In this paper, we introduce UnrealText, an efficient image synthesis method that renders realistic images via a 3D graphics engine. 3D synthetic engine provides realistic appearance by rendering scene and text as a whole, and allows for better text region proposals with access to precise scene information, e.g. normal and even object meshes. The comprehensive experiments verify its effectiveness on both scene text detection and recognition. We also generate a multilingual version for future research into multilingual scene text detection and recognition. Additionally, we re-annotate scene text recognition datasets in a case-sensitive way and include punctuation marks for more comprehensive evaluations. The code and the generated datasets are released at: this https URL .},
author = {Shangbang Long and C. Yao},
journal = {ArXiv},
volume = {abs/2003.10608},
pages = {null},
arxivid = {2003.10608},
}

@article{188feb685237ffbfd50aa2419e5ae30381cc967d,
title = {EraseNet: End-to-End Text Removal in the Wild},
year = {2020},
url = {https://www.semanticscholar.org/paper/188feb685237ffbfd50aa2419e5ae30381cc967d},
abstract = {Scene text removal has attracted increasing research interests owing to its valuable applications in privacy protection, camera-based virtual reality translation, and image editing. However, existing approaches, which fall short on real applications, are mainly because they were evaluated on synthetic or unrepresentative datasets. To fill this gap and facilitate this research direction, this article proposes a real-world dataset called SCUT-EnsText that consists of 3,562 diverse images selected from public scene text reading benchmarks, and each image is scrupulously annotated to provide visually plausible erasure targets. With SCUT-EnsText, we design a novel GAN-based model termed EraseNet that can automatically remove text located on the natural images. The model is a two-stage network that consists of a coarse-erasure sub-network and a refinement sub-network. The refinement sub-network targets improvement in the feature representation and refinement of the coarse outputs to enhance the removal performance. Additionally, EraseNet contains a segmentation head for text perception and a local-global SN-Patch-GAN with spectral normalization (SN) on both the generator and discriminator for maintaining the training stability and the congruity of the erased regions. A sufficient number of experiments are conducted on both the previous public dataset and the brand-new SCUT-EnsText. Our EraseNet significantly outperforms the existing state-of-the-art methods in terms of all metrics, with remarkably superior higher-quality results. The dataset and code will be made available at https://github.com/HCIILAB/SCUT-EnsText.},
author = {Chongyu Liu and Yuliang Liu and Lianwen Jin and Shuaitao Zhang and Canjie Luo and Yongpan Wang},
journal = {IEEE Transactions on Image Processing},
volume = {29},
pages = {8760-8775},
doi = {10.1109/TIP.2020.3018859},
pmid = {32857697},
}

@article{b8459104c93a493a598ebb21d5cf2cc8ff27bc5e,
title = {MTRNet: A Generic Scene Text Eraser},
year = {2019},
url = {https://www.semanticscholar.org/paper/b8459104c93a493a598ebb21d5cf2cc8ff27bc5e},
abstract = {Text removal algorithms have been proposed for uni-lingual scripts with regular shapes and layouts. However, to the best of our knowledge, a generic text removal method which is able to remove all or user-specified text regions regardless of font, script, language or shape is not available. Developing such a generic text eraser for real scenes is a challenging task, since it inherits all the challenges of multi-lingual and curved text detection and inpainting. To fill this gap, we propose a mask-based text removal network (MTRNet). MTRNet is a conditional adversarial generative network (cGAN) with an auxiliary mask. The introduced auxiliary mask not only makes the cGAN a generic text eraser, but also enables stable training and early convergence on a challenging large-scale synthetic dataset, initially proposed for text detection in real scenes. What's more, MTRNet achieves state-of-the-art results on several real-world datasets including ICDAR 2013, ICDAR 2017 MLT, and CTW1500, without being explicitly trained on this data, outperforming previous state-of-the-art methods trained directly on these datasets.},
author = {Osman Tursun and Rui Zeng and S. Denman and Sabesan Sivipalan and S. Sridharan and C. Fookes},
journal = {2019 International Conference on Document Analysis and Recognition (ICDAR)},
volume = {null},
pages = {39-44},
doi = {10.1109/ICDAR.2019.00016},
arxivid = {1903.04092},
}

@article{f207c2e951fa7f7c72adcb881e6e4a74046de43c,
title = {FET-GAN: Font and Effect Transfer via K-shot Adaptive Instance Normalization},
year = {2020},
url = {https://www.semanticscholar.org/paper/f207c2e951fa7f7c72adcb881e6e4a74046de43c},
abstract = {Text effect transfer aims at learning the mapping between text visual effects while maintaining the text content. While remarkably successful, existing methods have limited robustness in font transfer and weak generalization ability to unseen effects. To address these problems, we propose FET-GAN, a novel end-to-end framework to implement visual effects transfer with font variation among multiple text effects domains. Our model achieves remarkable results both on arbitrary effect transfer between texts and effect translation from text to graphic objects. By a few-shot fine-tuning strategy, FET-GAN can generalize the transfer of the pre-trained model to the new effect. Through extensive experimental validation and comparison, our model advances the state-of-the-art in the text effect transfer task. Besides, we have collected a font dataset including 100 fonts of more than 800 Chinese and English characters. Based on this dataset, we demonstrated the generalization ability of our model by the application that complements the font library automatically by few-shot samples. This application is significant in reducing the labor cost for the font designer.},
author = {Wei Li and Yongxing He and Yanwei Qi and Z. Li and Yongchuan Tang},
doi = {10.1609/AAAI.V34I02.5535},
}

@article{dcccc7b1be47e74700537a8303319e0d88fe75d3,
title = {Natural Scene Text Editing Based on AI},
year = {2021},
url = {https://www.semanticscholar.org/paper/dcccc7b1be47e74700537a8303319e0d88fe75d3},
abstract = {In natural photography, text editing is used to replace or update a word in an image while keeping the image's authenticity. The ability to edit text directly on photographs has a number of benefits, including the ability to correct mistakes and restore text. This is tough since the background and text styles of the altered image must match those of the original. This research shows how to change image text at the letter and digits level. I devised a two-part letters-digits network (LDN) to encode and decode digital images, as well as learn and transfer the font style of the source characters to the target characters. This method allows you to update the uppercase letters, lowercase letters and digits in the picture.},
author = {Yujie Zhang},
journal = {ArXiv},
volume = {abs/2111.15475},
pages = {null},
arxivid = {2111.15475},
}

@article{2a6a4cd7623d12b467571461e8c19a3138474908,
title = {Edit Probability for Scene Text Recognition},
year = {2018},
url = {https://www.semanticscholar.org/paper/2a6a4cd7623d12b467571461e8c19a3138474908},
abstract = {We consider the scene text recognition problem under the attention-based encoder-decoder framework, which is the state of the art. The existing methods usually employ a frame-wise maximal likelihood loss to optimize the models. When we train the model, the misalignment between the ground truth strings and the attention's output sequences of probability distribution, which is caused by missing or superfluous characters, will confuse and mislead the training process, and consequently make the training costly and degrade the recognition accuracy. To handle this problem, we propose a novel method called edit probability (EP) for scene text recognition. EP tries to effectively estimate the probability of generating a string from the output sequence of probability distribution conditioned on the input image, while considering the possible occurrences of missing/superfluous characters. The advantage lies in that the training process can focus on the missing, superfluous and unrecognized characters, and thus the impact of the misalignment problem can be alleviated or even overcome. We conduct extensive experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets. Experimental results show that the EP can substantially boost scene text recognition performance.},
author = {Fan Bai and Zhanzhan Cheng and Yi Niu and Shiliang Pu and Shuigeng Zhou},
journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
volume = {null},
pages = {1508-1516},
doi = {10.1109/CVPR.2018.00163},
arxivid = {1805.03384},
}

@article{65a278fa44b94683a028ac03622b28112e52f02a,
title = {STEFANN: Scene Text Editor Using Font Adaptive Neural Network},
year = {2019},
url = {https://www.semanticscholar.org/paper/65a278fa44b94683a028ac03622b28112e52f02a},
abstract = {Textual information in a captured scene plays an important role in scene interpretation and decision making. Though there exist methods that can successfully detect and interpret complex text regions present in a scene, to the best of our knowledge, there is no significant prior work that aims to modify the textual information in an image. The ability to edit text directly on images has several advantages including error correction, text restoration and image reusability. In this paper, we propose a method to modify text in an image at character-level. We approach the problem in two stages. At first, the unobserved character (target) is generated from an observed character (source) being modified. We propose two different neural network architectures - (a) FANnet to achieve structural consistency with source font and (b) Colornet to preserve source color. Next, we replace the source character with the generated character maintaining both geometric and visual consistency with neighboring characters. Our method works as a unified platform for modifying text in images. We present the effectiveness of our method on COCO-Text and ICDAR datasets both qualitatively and quantitatively.},
author = {Prasun Roy and Saumik Bhattacharya and Subhankar Ghosh and U. Pal},
journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
volume = {null},
pages = {13225-13234},
doi = {10.1109/cvpr42600.2020.01324},
arxivid = {1903.01192},
}

@article{84bb119053b4801f0e5da896d065b5eb606eb913,
title = {EnsNet: Ensconce Text in the Wild},
year = {2018},
url = {https://www.semanticscholar.org/paper/84bb119053b4801f0e5da896d065b5eb606eb913},
abstract = {A new method is proposed for removing text from natural images. The challenge is to first accurately localize text on the stroke-level and then replace it with a visually plausible background. Unlike previous methods that require image patches to erase scene text, our method, namely ensconce network (EnsNet), can operate end-to-end on a single image without any prior knowledge. The overall structure is an end-to-end trainable FCN-ResNet-18 network with a conditional generative adversarial network (cGAN). The feature of the former is first enhanced by a novel lateral connection structure and then refined by four carefully designed losses: multiscale regression loss and content loss, which capture the global discrepancy of different level features; texture loss and total variation loss, which primarily target filling the text region and preserving the reality of the background. The latter is a novel local-sensitive GAN, which attentively assesses the local consistency of the text erased regions. Both qualitative and quantitative sensitivity experiments on synthetic images and the ICDAR 2013 dataset demonstrate that each component of the EnsNet is essential to achieve a good performance. Moreover, our EnsNet can significantly outperform previous state-of-the-art methods in terms of all metrics. In addition, a qualitative experiment conducted on the SBMNet dataset further demonstrates that the proposed method can also preform well on general object (such as pedestrians) removal tasks. EnsNet is extremely fast, which can preform at 333 fps on an i5-8600 CPU device.},
author = {Shuaitao Zhang and Yuliang Liu and Lianwen Jin and Yaoxiong Huang and Songxuan Lai},
doi = {10.1609/aaai.v33i01.3301801},
arxivid = {1812.00723},
}

@article{e326b6bbbe9412fbc64f321db5651b51282ec1ac,
title = {Mask-guided GAN for robust text editing in the scene},
year = {2021},
url = {https://www.semanticscholar.org/paper/e326b6bbbe9412fbc64f321db5651b51282ec1ac},
abstract = {S2 TL;DR: A mask-guided GAN method is proposed to adequately use the body, outline, and shadow of text to guide the task, decomposing the complex task into easy-to-learn ones.},
author = {Boxi Yu and Yong Xu and Yan Huang and Shuai Yang and Jiaying Liu},
journal = {Neurocomputing},
volume = {441},
pages = {192-201},
doi = {10.1016/J.NEUCOM.2021.02.045},
}

@article{1a5bcec6bbb8bb3abd4121543b759c56547c0a8c,
title = {Two-Stage Seamless Text Erasing On Real-World Scene Images},
year = {2021},
url = {https://www.semanticscholar.org/paper/1a5bcec6bbb8bb3abd4121543b759c56547c0a8c},
abstract = {Erasing text from images is a common image-editing task in film industry and shared media. Existing text-erasing models either tend to produce artifacts or fail to remove all the text in real-world images. In this paper, we follow a two-stage text erasing framework that first masks the text by segmentation, and then inpaints the masked region to create a text-erased image. Our proposed text mask generator is designed to accurately cover text, which combined with inpainting, can produce reliable text-erased results. In the inpainting model, we propose a Multiscale Gradient Reconstruction Loss to generate sharp realistic-looking images. Our model achieves state-of-the-art results on both synthetic and real world data in both quantitative and qualitative measures.},
author = {Benjamin Conrad and P. Chen},
journal = {2021 IEEE International Conference on Image Processing (ICIP)},
volume = {null},
pages = {1309-1313},
doi = {10.1109/ICIP42928.2021.9506394},
}

@article{7b39a42c7974e06063029ad48a4a1981fcafc845,
title = {Editing Text in the Wild},
year = {2019},
url = {https://www.semanticscholar.org/paper/7b39a42c7974e06063029ad48a4a1981fcafc845},
abstract = {In this paper, we are interested in editing text in natural images, which aims to replace or modify a word in the source image with another one while maintaining its realistic look. This task is challenging, as the styles of both background and text need to be preserved so that the edited image is visually indistinguishable from the source image. Specifically, we propose an end-to-end trainable style retention network (SRNet) that consists of three modules: text conversion module, background inpainting module and fusion module. The text conversion module changes the text content of the source image into the target text while keeping the original text style. The background inpainting module erases the original text, and fills the text region with appropriate texture. The fusion module combines the information from the two former modules, and generates the edited text images. To our knowledge, this work is the first attempt to edit text in natural images at the word level. Both visual effects and quantitative results on synthetic and real-world dataset (ICDAR 2013) fully confirm the importance and necessity of modular decomposition. We also conduct extensive experiments to validate the usefulness of our method in various real-world applications such as text image synthesis, augmented reality (AR) translation, information hiding, etc.},
author = {Liang Wu and Chengquan Zhang and Jiaming Liu and Junyu Han and Jingtuo Liu and Errui Ding and X. Bai},
journal = {Proceedings of the 27th ACM International Conference on Multimedia},
volume = {null},
pages = {null},
doi = {10.1145/3343031.3350929},
arxivid = {1908.03047},
}

@article{527e705c3ba8ed1eafb76c2a71b3a9e6a9be3c58,
title = {SKFont: skeleton-driven Korean font generator with conditional deep adversarial networks},
year = {2021},
url = {https://www.semanticscholar.org/paper/527e705c3ba8ed1eafb76c2a71b3a9e6a9be3c58},
abstract = {S2 TL;DR: Qualitative and quantitative comparisons with the state-of-the-art methods demonstrate the superiority of the proposed SKFont method, which resolves long overdue shortfalls such as blurriness, breaking, and a lack of delivery of delicate shapes and styles by using the ‘skeleton-driven’ conditional deep adversarial network.},
author = {Debbie Honghee Ko and Ammar Ul Hassan and J. Suk and Jaeyoung Choi},
journal = {International Journal on Document Analysis and Recognition (IJDAR)},
volume = {24},
pages = {325 - 337},
doi = {10.1007/s10032-021-00374-4},
}

@article{fc59e1ee5a2d9cefbdf1b7954cc419b571a59221,
title = {Few-shot Font Style Transfer between Different Languages},
year = {2021},
url = {https://www.semanticscholar.org/paper/fc59e1ee5a2d9cefbdf1b7954cc419b571a59221},
abstract = {In this paper, we propose a novel model FTransGAN that can transfer font styles between different languages by observing only a few samples. The automatic generation of a new font library is a challenging task and has been attracting many researchers’ interests. Most previous works addressed this problem by transferring the style of the given subset to the content of unseen ones. Nevertheless, they only focused on the font style transfer in the same language. In many tasks, we need to learn the font information from one language and then apply it to other languages. It’s difficult for the existing methods to do such tasks. To solve this problem, we specifically design our network into a multi-level attention form to capture both local and global features of the style images. To verify the generative ability of our model, we construct an experimental font dataset which includes 847 fonts, each of them containing English and Chinese characters with the same style. Experimental results show that compared with the state-of-the-art models, our model generates 80.3% of all user preferred images.},
author = {Chenhao Li and Yuta Taniguchi and M. Lu and S. Konomi},
journal = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
volume = {null},
pages = {433-442},
doi = {10.1109/WACV48630.2021.00048},
}

@article{2f652a342f2c539c4f4dd8ea824fd4da88493915,
title = {TET-GAN: Text Effects Transfer via Stylization and Destylization},
year = {2018},
url = {https://www.semanticscholar.org/paper/2f652a342f2c539c4f4dd8ea824fd4da88493915},
abstract = {Text effects transfer technology automatically makes the text dramatically more impressive. However, previous style transfer methods either study the model for general style, which cannot handle the highly-structured text effects along the glyph, or require manual design of subtle matching criteria for text effects. In this paper, we focus on the use of the powerful representation abilities of deep neural features for text effects transfer. For this purpose, we propose a novel Texture Effects Transfer GAN (TET-GAN), which consists of a stylization subnetwork and a destylization subnetwork. The key idea is to train our network to accomplish both the objective of style transfer and style removal, so that it can learn to disentangle and recombine the content and style features of text effects images. To support the training of our network, we propose a new text effects dataset with as much as 64 professionally designed styles on 837 characters. We show that the disentangled feature representations enable us to transfer or remove all these styles on arbitrary glyphs using one network. Furthermore, the flexible network design empowers TET-GAN to efficiently extend to a new text style via oneshot learning where only one example is required. We demonstrate the superiority of the proposed method in generating high-quality stylized text over the state-of-the-art methods.},
author = {Shuai Yang and Jiaying Liu and Wenjing Wang and Zongming Guo},
doi = {10.1609/AAAI.V33I01.33011238},
arxivid = {1812.06384},
}

@article{77d65560f0ee09bba56275469dba73d4eda73892,
title = {Learning to Generate Realistic Scene Chinese Character Images by Multitask Coupled GAN},
year = {2018},
url = {https://www.semanticscholar.org/paper/77d65560f0ee09bba56275469dba73d4eda73892},
abstract = {S2 TL;DR: Experiments show that the proposed MtC-GAN framework is general and flexible to improve the accuracy for SCCR and train the multitask networks using a new loss function that combines the constrains of encoders, generators and classifiers simultaneously.},
author = {Qingxiang Lin and Lingyu Liang and Yaoxiong Huang and Lianwen Jin},
doi = {10.1007/978-3-030-03338-5_4},
}

@article{57153f1bccc69e070173d2a540a0b1a550e01c5d,
title = {Alchemy: Techniques for Rectification Based Irregular Scene Text Recognition},
year = {2019},
url = {https://www.semanticscholar.org/paper/57153f1bccc69e070173d2a540a0b1a550e01c5d},
abstract = {Reading text from natural images is challenging due to the great variety in text font, color, size, complex background and etc.. The perspective distortion and non-linear spatial arrangement of characters make it further difficult. While rectification based method is intuitively grounded and has pushed the envelope by far, its potential is far from being well exploited. In this paper, we present a bag of techniques that prove to significantly improve the performance of rectification based method. On curved text dataset, our method achieves an accuracy of 89.6% on CUTE80 and 76.3% on Total-Text, an improvement over previous state-of-the-art by 6.3% and 14.7% respectively. Furthermore, our combination of techniques helps us win the ICDAR 2019 ArbitraryShaped Text Challenge (Latin script), achieving an accuracy of 74.3% on the held-out test set. We release our code as well as data samples for further exploration at https://github.com/Jyouhou/ ICDAR2019-ArT-Recognition-Alchemy.},
author = {Shangbang Long and Yushuo Guan and Bingxuan Wang and Kaigui Bian and C. Yao},
journal = {ArXiv},
volume = {abs/1908.11834},
pages = {null},
}

@article{d1e356ac88768b3cb5b59926641b48cbaa15670c,
title = {STAR-Net: A SpaTial Attention Residue Network for Scene Text Recognition},
year = {2016},
url = {https://www.semanticscholar.org/paper/d1e356ac88768b3cb5b59926641b48cbaa15670c},
abstract = {In this paper, we present a novel SpaTial Attention Residue Network (STAR-Net) for recognising scene texts. The overall architecture of our STAR-Net is illustrated in fig. 1. Our STARNet emphasises the importance of representative image-based feature extraction from text regions by the spatial attention mechanism and the residue learning strategy. It is by far the deepest neural network proposed for scene text recognition.},
author = {W. Liu and Chaofeng Chen and Kwan-Yee Kenneth Wong and Zhizhong Su and Junyu Han},
doi = {10.5244/C.30.43},
}

@article{8a8bac2928ea89f64103075770890981919de262,
title = {RewriteNet: Realistic Scene Text Image Generation via Editing Text in Real-world Image},
year = {2021},
url = {https://www.semanticscholar.org/paper/8a8bac2928ea89f64103075770890981919de262},
abstract = {Scene text editing (STE), which converts a text in a scene image into the desired text while preserving an original style, is a challenging task due to a complex intervention between text and style. To address this challenge, we propose a novel representational learning-based STE model, referred to as RewriteNet that employs textual information as well as visual information. We assume that the scene text image can be decomposed into content and style features where the former represents the text information and style represents scene text characteristics such as font, alignment, and background. Under this assumption, we propose a method to separately encode content and style features of the input image by introducing the scene text recognizer that is trained by text information. Then, a text-edited image is generated by combining the style feature from the original image and the content feature from the target text. Unlike previous works that are only able to use synthetic images in the training phase, we also exploit real-world images by proposing a self-supervised training scheme, which bridges the domain gap between synthetic and real data. Our experiments demonstrate that RewriteNet achieves better quantitative and qualitative performance than other comparisons. Moreover, we validate that the use of text information and the self-supervised training scheme improves text switching performance. The implementation and dataset will be publicly available. * indicates equal contribution. † indicates corresponding author.},
author = {Junyeop Lee and Yoonsik Kim and Seonghyeon Kim and Moonbin Yim and Seung Shin and Gayoung Lee and Sungrae Park},
journal = {ArXiv},
volume = {abs/2107.11041},
pages = {null},
}

@article{2e50ff82a48f24acff2120a2f91ff84a1272a47b,
title = {Few-shot Compositional Font Generation with Dual Memory},
year = {2020},
url = {https://www.semanticscholar.org/paper/2e50ff82a48f24acff2120a2f91ff84a1272a47b},
abstract = {S2 TL;DR: This paper proposes a novel font generation framework, named Dual Memory-augmented Font Generation Network (DM-Font), which enables us to generate a high-quality font library with only a few samples, and employs memory components and global-context awareness in the generator to take advantage of the compositionality.},
author = {Junbum Cha and Sanghyuk Chun and Gayoung Lee and Bado Lee and Seonghyeon Kim and Hwalsuk Lee},
doi = {10.1007/978-3-030-58529-7_43},
arxivid = {2005.10510},
}

@article{bbb71cbca731295758563acdc67273b99618e1c0,
title = {SwapNet: Image Based Garment Transfer},
year = {2018},
url = {https://www.semanticscholar.org/paper/bbb71cbca731295758563acdc67273b99618e1c0},
abstract = {S2 TL;DR: This work presents Swapnet, a framework to transfer garments across images of people with arbitrary body pose, shape, and clothing, and proposes a novel weakly-supervised approach that generates training pairs from a single image via data augmentation.},
author = {Amit Raj and Patsorn Sangkloy and Huiwen Chang and James Hays and Duygu Ceylan and Jingwan Lu},
doi = {10.1007/978-3-030-01258-8_41},
}

@article{b7fecd549152c7f7d7aa1364353cdbcc0e1ea3bc,
title = {Erasing Scene Text with Weak Supervision},
year = {2020},
url = {https://www.semanticscholar.org/paper/b7fecd549152c7f7d7aa1364353cdbcc0e1ea3bc},
abstract = {Scene text erasing is a task of removing text from natural scene images, which has been gaining attention in recent years. The main motivation is to conceal private information such as license plate numbers, and house nameplates that can appear in images. In this work, we propose a method for scene text erasing that approaches the problem as a general inpainting task. In contrast to previous methods, which require pairs of original images containing text and images from which the text has been removed, our method does not need corresponding image pairs for training. We use a separately trained scene text detector and an inpainting network. The scene text detector predicts segmentation maps of text instances which are then used as masks for the inpainting network. The network for inpainting, trained on a large-scale image dataset, fills in masked out regions in an input image and generates a final image in which the original text is no longer present. The results show that our method is able to successfully remove text and fill in the created holes to produce natural-looking images.},
author = {Jan Zdenek and Hideki Nakayama},
journal = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
volume = {null},
pages = {2227-2235},
doi = {10.1109/WACV45572.2020.9093544},
}

@article{6749f4827700b3ba375d46a8bff421685d522f18,
title = {Rethinking Irregular Scene Text Recognition},
year = {2019},
url = {https://www.semanticscholar.org/paper/6749f4827700b3ba375d46a8bff421685d522f18},
abstract = {Reading text from natural images is challenging due to the great variety in text font, color, size, complex background and etc.. The perspective distortion and non-linear spatial arrangement of characters make it further difficult. While rectification based method is intuitively grounded and has pushed the envelope by far, its potential is far from being well exploited. In this paper, we present a bag of tricks that prove to significantly improve the performance of rectification based method. On curved text dataset, our method achieves an accuracy of 89.6% on CUTE-80 and 76.3% on Total-Text, an improvement over previous state-of-the-art by 6.3% and 14.7% respectively. Furthermore, our combination of tricks helps us win the ICDAR 2019 Arbitrary-Shaped Text Challenge (Latin script), achieving an accuracy of 74.3% on the held-out test set. We release our code as well as data samples for further exploration at this https URL},
author = {Shangbang Long and Yushuo Guan and Bingxuan Wang and Kaigui Bian and C. Yao},
journal = {arXiv: Computer Vision and Pattern Recognition},
volume = {},
pages = {null},
arxivid = {1908.11834},
}

@article{68170ee9dec3c6108194b4f294d9c00119ad5de0,
title = {MTRNet++: One-stage Mask-based Scene Text Eraser},
year = {2019},
url = {https://www.semanticscholar.org/paper/68170ee9dec3c6108194b4f294d9c00119ad5de0},
abstract = {S2 TL;DR: The results of ablation studies demonstrate that the proposed multi-branch architecture with attention blocks is effective and essential, and demonstrates controllability and interpretability.},
author = {Osman Tursun and S. Denman and Rui Zeng and Sabesan Sivapalan and S. Sridharan and C. Fookes},
journal = {Comput. Vis. Image Underst.},
volume = {201},
pages = {103066},
doi = {10.1016/J.CVIU.2020.103066},
arxivid = {1912.07183},
}

@article{493a7272d8b04f4e06344f4b4058d2b99fc0ebef,
title = {Synthetically Supervised Feature Learning for Scene Text Recognition},
year = {2018},
url = {https://www.semanticscholar.org/paper/493a7272d8b04f4e06344f4b4058d2b99fc0ebef},
abstract = {S2 TL;DR: This work designs a multi-task network with an encoder-discriminator-generator architecture to guide the feature of the original image toward that of the clean image, and significantly outperforms the state-of-the-art methods on standard scene text recognition benchmarks in the lexicon-free category.},
author = {Yang Liu and Zhaowen Wang and Hailin Jin and I. Wassell},
doi = {10.1007/978-3-030-01228-1_27},
}

@article{9828a6b90e1608172c3ded468afa47a8ab51fbc8,
title = {Exploring Font-independent Features for Scene Text Recognition},
year = {2020},
url = {https://www.semanticscholar.org/paper/9828a6b90e1608172c3ded468afa47a8ab51fbc8},
abstract = {Scene text recognition (STR) has been extensively studied in last few years. Many recently-proposed methods are specially designed to accommodate the arbitrary shape, layout and orientation of scene texts, but ignoring that various font (or writing) styles also pose severe challenges to STR. These methods, where font features and content features of characters are tangled, perform poorly in text recognition on scene images with texts in novel font styles. To address this problem, we explore font-independent features of scene texts via attentional generation of glyphs in a large number of font styles. Specifically, we introduce trainable font embeddings to shape the font styles of generated glyphs, with the image feature of scene text only representing its essential patterns. The generation process is directed by the spatial attention mechanism, which effectively copes with irregular texts and generates higher-quality glyphs than existing image-to-image translation methods. Experiments conducted on several STR benchmarks demonstrate the superiority of our method compared to the state of the art.},
author = {Yizhi Wang and Z. Lian},
journal = {Proceedings of the 28th ACM International Conference on Multimedia},
volume = {null},
pages = {null},
doi = {10.1145/3394171.3413592},
arxivid = {2009.07447},
}

@article{5b09a636feafcddc02d747d3a1fc24850a8ac5d2,
title = {Multi-scale Attention Guided Pose Transfer},
year = {2022},
url = {https://www.semanticscholar.org/paper/5b09a636feafcddc02d747d3a1fc24850a8ac5d2},
abstract = {S2 TL;DR: An improved network architecture for pose transfer is presented by introducing attention links at every resolution level of the encoder and decoder by utilizing such dense multi-scale attention guided approach to achieve significant improvement over the existing methods both visually and analytically.},
author = {Prasun Roy and Saumik Bhattacharya and Subhankar Ghosh and U. Pal},
journal = {ArXiv},
volume = {abs/2202.06777},
pages = {null},
doi = {10.1016/j.patcog.2023.109315},
arxivid = {2202.06777},
}

@article{f018bba9b85e7a43e10ae48b1dd2890dbb801de3,
title = {Stroke-Based Scene Text Erasing Using Synthetic Data for Training},
year = {2021},
url = {https://www.semanticscholar.org/paper/f018bba9b85e7a43e10ae48b1dd2890dbb801de3},
abstract = {Scene text erasing, which replaces text regions with reasonable content in natural images, has drawn significant attention in the computer vision community in recent years. There are two potential subtasks in scene text erasing: text detection and image inpainting. Both subtasks require considerable data to achieve better performance; however, the lack of a large-scale real-world scene-text removal dataset does not allow existing methods to realize their potential. To compensate for the lack of pairwise real-world data, we made considerable use of synthetic text after additional enhancement and subsequently trained our model only on the dataset generated by the improved synthetic text engine. Our proposed network contains a stroke mask prediction module and background inpainting module that can extract the text stroke as a relatively small hole from the cropped text image to maintain more background content for better inpainting results. This model can partially erase text instances in a scene image with a bounding box or work with an existing scene-text detector for automatic scene text erasing. The experimental results from the qualitative and quantitative evaluation on the SCUT-Syn, ICDAR2013, and SCUT-EnsText datasets demonstrate that our method significantly outperforms existing state-of-the-art methods even when they are trained on real-world data.},
author = {Zhengmi Tang and Tomo Miyazaki and Yoshihiro Sugaya and S. Omachi},
journal = {IEEE Transactions on Image Processing},
volume = {30},
pages = {9306-9320},
doi = {10.1109/TIP.2021.3125260},
pmid = {34752394},
arxivid = {2104.11493},
}

@article{c891910b0f995994a614e9eae875f5d182948b95,
title = {Scene Text Recognition from Two-Dimensional Perspective},
year = {2018},
url = {https://www.semanticscholar.org/paper/c891910b0f995994a614e9eae875f5d182948b95},
abstract = {Inspired by speech recognition, recent state-of-the-art algorithms mostly consider scene text recognition as a sequence prediction problem. Though achieving excellent performance, these methods usually neglect an important fact that text in images are actually distributed in two-dimensional space. It is a nature quite different from that of speech, which is essentially a one-dimensional signal. In principle, directly compressing features of text into a one-dimensional form may lose useful information and introduce extra noise. In this paper, we approach scene text recognition from a two-dimensional perspective. A simple yet effective model, called Character Attention Fully Convolutional Network (CA-FCN), is devised for recognizing the text of arbitrary shapes. Scene text recognition is realized with a semantic segmentation network, where an attention mechanism for characters is adopted. Combined with a word formation module, CA-FCN can simultaneously recognize the script and predict the position of each character. Experiments demonstrate that the proposed algorithm outperforms previous methods on both regular and irregular text datasets. Moreover, it is proven to be more robust to imprecise localizations in the text detection phase, which are very common in practice.},
author = {Minghui Liao and Jian Zhang and Zhaoyi Wan and Fengming Xie and Jiajun Liang and Pengyuan Lyu and C. Yao and X. Bai},
doi = {10.1609/aaai.v33i01.33018714},
arxivid = {1809.06508},
}

@article{59102def1887f161f05e85a5224e190b20b3fc7b,
title = {Separating Content from Style Using Adversarial Learning for Recognizing Text in the Wild},
year = {2021},
url = {https://www.semanticscholar.org/paper/59102def1887f161f05e85a5224e190b20b3fc7b},
abstract = {S2 TL;DR: This work proposes an adversarial learning framework for the generation and recognition of multiple characters in an image, which consists of an attention-based recognizer and a generative adversarial architecture, and designs an interactive joint training scheme to tackle the issue of lacking paired training samples.},
author = {Canjie Luo and Qingxiang Lin and Yuliang Liu and Lianwen Jin and Chunhua Shen},
journal = {International Journal of Computer Vision},
volume = {129},
pages = {960 - 976},
doi = {10.1007/s11263-020-01411-1},
arxivid = {2001.04189},
}

@article{f5eec0ff44d936701e3a769917f03b8a60af9086,
title = {Deep Style Transfer for Line Drawings},
year = {2021},
url = {https://www.semanticscholar.org/paper/f5eec0ff44d936701e3a769917f03b8a60af9086},
abstract = {Line drawings are frequently used to illustrate ideas and concepts in digital documents and presentations. To compose a line drawing, it is common for users to retrieve multiple line drawings from the Internet and combine them as one image. However, different line drawings may have different line styles and are visually inconsistent when put together. In order that the line drawings can have consistent looks, in this paper, we make the first attempt to perform style transfer for line drawings. The key of our design lies in the fact that centerline plays a very important role in preserving line topology and extracting style features. With this finding, we propose to formulate the style transfer problem as a centerline stylization problem and solve it via a novel style-guided image-to-image translation network. Results and statistics show that our method significantly outperforms the existing methods both visually and quantitatively.},
author = {Xueting Liu and Wenliang Wu and Huisi Wu and Zhenkun Wen},
doi = {10.1609/aaai.v35i1.16111},
}

@article{402b071589b5c20bb42bb609298e40c5b3623d1d,
title = {Generating Text Sequence Images for Recognition},
year = {2019},
url = {https://www.semanticscholar.org/paper/402b071589b5c20bb42bb609298e40c5b3623d1d},
abstract = {S2 TL;DR: This work presents a method which is able to generate infinite training data without any auxiliary pre/post-process and utilizes conditional adversarial networks to produce realistic text sequence images in the light of the semantic ones.},
author = {Yanxiang Gong and Linjie Deng and Z. Ma and M. Xie},
journal = {Neural Processing Letters},
volume = {51},
pages = {1677-1688},
doi = {10.1007/s11063-019-10166-x},
arxivid = {1901.06782},
}

@article{5c26e8fa5ffadaf410387082ed1bdf3ee104f8f7,
title = {AccNet: occluded scene text enhancing network with accretion blocks},
year = {2022},
url = {https://www.semanticscholar.org/paper/5c26e8fa5ffadaf410387082ed1bdf3ee104f8f7},
abstract = {S2 TL;DR: This work discusses different kinds of occlusions and proposes an occluded scene text enhancing network, based on generative adversarial networks, which can be readily used in different frameworks and can be easily trained without the annotations of text content.},
author = {Yanxiang Gong and Zhiqiang Zhang and Guozhen Duan and Zheng Ma and M. Xie},
journal = {Machine Vision and Applications},
volume = {34},
pages = {null},
doi = {10.1007/s00138-022-01351-5},
}

@article{1439cdd88b4d584fa2a0e7552f218b8ab932a2ff,
title = {Controllable Artistic Text Style Transfer via Shape-Matching GAN},
year = {2019},
url = {https://www.semanticscholar.org/paper/1439cdd88b4d584fa2a0e7552f218b8ab932a2ff},
abstract = {Artistic text style transfer is the task of migrating the style from a source image to the target text to create artistic typography. Recent style transfer methods have considered texture control to enhance usability. However, controlling the stylistic degree in terms of shape deformation remains an important open challenge. In this paper, we present the first text style transfer network that allows for real-time control of the crucial stylistic degree of the glyph through an adjustable parameter. Our key contribution is a novel bidirectional shape matching framework to establish an effective glyph-style mapping at various deformation levels without paired ground truth. Based on this idea, we propose a scale-controllable module to empower a single network to continuously characterize the multi-scale shape features of the style image and transfer these features to the target text. The proposed method demonstrates its superiority over previous state-of-the-arts in generating diverse, controllable and high-quality stylized text.},
author = {Shuai Yang and Zhangyang Wang and Zhaowen Wang and N. Xu and Jiaying Liu and Zongming Guo},
journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
volume = {null},
pages = {4441-4450},
doi = {10.1109/ICCV.2019.00454},
arxivid = {1905.01354},
}

@article{06bd18822bbcee974f006e0a0fdf89c57fea1480,
title = {Chinese Typeface Transformation with Hierarchical Adversarial Network},
year = {2017},
url = {https://www.semanticscholar.org/paper/06bd18822bbcee974f006e0a0fdf89c57fea1480},
abstract = {In this paper, we explore automated typeface generation through image style transfer which has shown great promise in natural image generation. Existing style transfer methods for natural images generally assume that the source and target images share similar high-frequency features. However, this assumption is no longer true in typeface transformation. Inspired by the recent advancement in Generative Adversarial Networks (GANs), we propose a Hierarchical Adversarial Network (HAN) for typeface transformation. The proposed HAN consists of two sub-networks: a transfer network and a hierarchical adversarial discriminator. The transfer network maps characters from one typeface to another. A unique characteristic of typefaces is that the same radicals may have quite different appearances in different characters even under the same typeface. Hence, a stage-decoder is employed by the transfer network to leverage multiple feature layers, aiming to capture both the global and local features. The hierarchical adversarial discriminator implicitly measures data discrepancy between the generated domain and the target domain. To leverage the complementary discriminating capability of different feature layers, a hierarchical structure is proposed for the discriminator. We have experimentally demonstrated that HAN is an effective framework for typeface transfer and characters restoration.},
author = {Jie Chang and Yujun Gu and Ya Zhang},
journal = {ArXiv},
volume = {abs/1711.06448},
pages = {null},
arxivid = {1711.06448},
}

@article{7fadb96f317cd3e1b7c5ef7990c5ac258e2bca30,
title = {AON: Towards Arbitrarily-Oriented Text Recognition},
year = {2017},
url = {https://www.semanticscholar.org/paper/7fadb96f317cd3e1b7c5ef7990c5ac258e2bca30},
abstract = {Recognizing text from natural images is a hot research topic in computer vision due to its various applications. Despite the enduring research of several decades on optical character recognition (OCR), recognizing texts from natural images is still a challenging task. This is because scene texts are often in irregular (e.g. curved, arbitrarily-oriented or seriously distorted) arrangements, which have not yet been well addressed in the literature. Existing methods on text recognition mainly work with regular (horizontal and frontal) texts and cannot be trivially generalized to handle irregular texts. In this paper, we develop the arbitrary orientation network (AON) to directly capture the deep features of irregular texts, which are combined into an attention-based decoder to generate character sequence. The whole network can be trained end-to-end by using only images and word-level annotations. Extensive experiments on various benchmarks, including the CUTE80, SVT-Perspective, IIIT5k, SVT and ICDAR datasets, show that the proposed AON-based method achieves the-state-of-the-art performance in irregular datasets, and is comparable to major existing methods in regular datasets.},
author = {Zhanzhan Cheng and Yangliu Xu and Fan Bai and Yi Niu and Shiliang Pu and Shuigeng Zhou},
journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
volume = {null},
pages = {5571-5579},
doi = {10.1109/CVPR.2018.00584},
}
