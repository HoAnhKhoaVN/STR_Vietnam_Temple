@article{84bb119053b4801f0e5da896d065b5eb606eb913,
title = {EnsNet: Ensconce Text in the Wild},
year = {2018},
url = {https://www.semanticscholar.org/paper/84bb119053b4801f0e5da896d065b5eb606eb913},
abstract = {A new method is proposed for removing text from natural images. The challenge is to first accurately localize text on the stroke-level and then replace it with a visually plausible background. Unlike previous methods that require image patches to erase scene text, our method, namely ensconce network (EnsNet), can operate end-to-end on a single image without any prior knowledge. The overall structure is an end-to-end trainable FCN-ResNet-18 network with a conditional generative adversarial network (cGAN). The feature of the former is first enhanced by a novel lateral connection structure and then refined by four carefully designed losses: multiscale regression loss and content loss, which capture the global discrepancy of different level features; texture loss and total variation loss, which primarily target filling the text region and preserving the reality of the background. The latter is a novel local-sensitive GAN, which attentively assesses the local consistency of the text erased regions. Both qualitative and quantitative sensitivity experiments on synthetic images and the ICDAR 2013 dataset demonstrate that each component of the EnsNet is essential to achieve a good performance. Moreover, our EnsNet can significantly outperform previous state-of-the-art methods in terms of all metrics. In addition, a qualitative experiment conducted on the SBMNet dataset further demonstrates that the proposed method can also preform well on general object (such as pedestrians) removal tasks. EnsNet is extremely fast, which can preform at 333 fps on an i5-8600 CPU device.},
author = {Shuaitao Zhang and Yuliang Liu and Lianwen Jin and Yaoxiong Huang and Songxuan Lai},
journal = {ArXiv},
volume = {abs/1812.00723},
pages = {null},
doi = {10.1609/aaai.v33i01.3301801},
arxivid = {1812.00723},
}

@article{0ff76d661822bed3e7b378cb8efb36c101359ede,
title = {Detecting and Removing Text in the Wild},
year = {2021},
url = {https://www.semanticscholar.org/paper/0ff76d661822bed3e7b378cb8efb36c101359ede},
abstract = {Scene text removal is a challenging task that aims to erase wild text regions that include text strokes and their ambiguous boundaries, such as embossing, shade, or flare. The challenging issues raised in the wild are not completely addressed by the existing methods. To address these issues, we propose a new loss function for blending two tasks in a new network structure that depicts wild text regions in a soft mask and selectively inpaints them into a sensible background. The proposed loss function aids the learning of two seemingly separate tasks in a synergistic way via the soft mask to achieve remarkable performance in scene text removal. We validate our method through qualitative and quantitative comparisons, and region-wise analysis, showing that our method outperforms existing methods.},
author = {Junho Cho and Sangdoo Yun and Dongyoon Han and Byeongho Heo and J. Choi},
journal = {IEEE Access},
volume = {9},
pages = {123313-123323},
doi = {10.1109/ACCESS.2021.3110293},
}

@article{fee6fb37ce240cc147bf88c37b6d9d7cfaf1edf4,
title = {ViTEraser: Harnessing the Power of Vision Transformers for Scene Text Removal with SegMIM Pretraining},
year = {2023},
url = {https://www.semanticscholar.org/paper/fee6fb37ce240cc147bf88c37b6d9d7cfaf1edf4},
abstract = {Scene text removal (STR) aims at replacing text strokes in natural scenes with visually coherent backgrounds. Recent STR approaches rely on iterative refinements or explicit text masks, resulting in higher complexity and sensitivity to the accuracy of text localization. Moreover, most existing STR methods utilize convolutional neural networks (CNNs) for feature representation while the potential of vision Transformers (ViTs) remains largely unexplored. In this paper, we propose a simple-yet-effective ViT-based text eraser, dubbed ViTEraser. Following a concise encoder-decoder framework, different types of ViTs can be easily integrated into ViTEraser to enhance the long-range dependencies and global reasoning. Specifically, the encoder hierarchically maps the input image into the hidden space through ViT blocks and patch embedding layers, while the decoder gradually upsamples the hidden features to the text-erased image with ViT blocks and patch splitting layers. As ViTEraser implicitly integrates text localization and inpainting, we propose a novel end-to-end pretraining method, termed SegMIM, which focuses the encoder and decoder on the text box segmentation and masked image modeling tasks, respectively. To verify the effectiveness of the proposed methods, we comprehensively explore the architecture, pretraining, and scalability of the ViT-based encoder-decoder for STR, which provides deep insights into the application of ViT to STR. Experimental results demonstrate that ViTEraser with SegMIM achieves state-of-the-art performance on STR by a substantial margin. Furthermore, the extended experiment on tampered scene text detection demonstrates the generality of ViTEraser to other tasks. We believe this paper can inspire more research on ViT-based STR approaches. Code will be available at https://github.com/shannanyinxiang/ViTEraser.},
author = {Dezhi Peng and Chongyu Liu and Yuliang Liu and Lianwen Jin},
journal = {ArXiv},
volume = {abs/2306.12106},
pages = {null},
doi = {10.48550/arXiv.2306.12106},
arxivid = {2306.12106},
}

@article{f600180634bc7e06ce9123cba7ed584905014242,
title = {Text localization, extraction and inpainting in color images},
year = {2012},
url = {https://www.semanticscholar.org/paper/f600180634bc7e06ce9123cba7ed584905014242},
abstract = {In this paper, we propose a new algorithm for subtitle text detection, extraction and text inpainting in color images. The proposed algorithm includes three stages. In the first stage, we localize text blocks to extract subtitles. We then extract the text characters precisely and in the third stage, we use an inpainting algorithm to recover the original texture of the image. To localize text blocks, we use stroke filter and new segmentation and verification algorithms based on the image profiles. To extract text character precisely, we estimate background and text color in the candidate blocks using the color histogram measured in different sub-blocks of the candidate text blocks. Finally, the inpainting algorithm based on matching algorithm is utilized to reconstruct the initial image contents in text areas. Our inpainting algorithm considers priority for inpainting of pixels, which results in perfect reconstruction in areas with texture variation. We tested the proposed algorithm with different image and video frames and compared the results with those of other methods. Experimental results showed the efficiency of the proposed algorithm.},
author = {Mohammad Khodadadi and A. Behrad},
journal = {20th Iranian Conference on Electrical Engineering (ICEE2012)},
volume = {null},
pages = {1035-1040},
doi = {10.1109/IRANIANCEE.2012.6292505},
}

@article{188feb685237ffbfd50aa2419e5ae30381cc967d,
title = {EraseNet: End-to-End Text Removal in the Wild},
year = {2020},
url = {https://www.semanticscholar.org/paper/188feb685237ffbfd50aa2419e5ae30381cc967d},
abstract = {Scene text removal has attracted increasing research interests owing to its valuable applications in privacy protection, camera-based virtual reality translation, and image editing. However, existing approaches, which fall short on real applications, are mainly because they were evaluated on synthetic or unrepresentative datasets. To fill this gap and facilitate this research direction, this article proposes a real-world dataset called SCUT-EnsText that consists of 3,562 diverse images selected from public scene text reading benchmarks, and each image is scrupulously annotated to provide visually plausible erasure targets. With SCUT-EnsText, we design a novel GAN-based model termed EraseNet that can automatically remove text located on the natural images. The model is a two-stage network that consists of a coarse-erasure sub-network and a refinement sub-network. The refinement sub-network targets improvement in the feature representation and refinement of the coarse outputs to enhance the removal performance. Additionally, EraseNet contains a segmentation head for text perception and a local-global SN-Patch-GAN with spectral normalization (SN) on both the generator and discriminator for maintaining the training stability and the congruity of the erased regions. A sufficient number of experiments are conducted on both the previous public dataset and the brand-new SCUT-EnsText. Our EraseNet significantly outperforms the existing state-of-the-art methods in terms of all metrics, with remarkably superior higher-quality results. The dataset and code will be made available at https://github.com/HCIILAB/SCUT-EnsText.},
author = {Chongyu Liu and Yuliang Liu and Lianwen Jin and Shuaitao Zhang and Canjie Luo and Yongpan Wang},
journal = {IEEE Transactions on Image Processing},
volume = {29},
pages = {8760-8775},
doi = {10.1109/TIP.2020.3018859},
pmid = {32857697},
}

@article{ed59106df85bec9bf63f8ee507abfb5554fa85b4,
title = {Text removal network based on comprehensive loss evaluation},
year = {2021},
url = {https://www.semanticscholar.org/paper/ed59106df85bec9bf63f8ee507abfb5554fa85b4},
abstract = {This paper proposes a text removal model, Text Remove Network (TRNet), which achieves an unprecedented clearing effect of picture text. The network uses a jump-connected U-net structure to encode and decode the generator, so as to obtain clearer and sharper texture details of the original image. To solve the problem of color distortion, the generator removes the batch normalization layer and uses ELUs as the activation layer of all convolutional layers. Through the comprehensive loss, which include reconstruction, content, style, total variation, and structural similarity (SSIM) loss, we can clear the image text and preserve the image information of the background, which solves the problem of incomplete text removal and loss of background texture. The local discriminator is used to evaluate the local consistency of the text erasure area. In a text elimination experiment on a synthetic dataset and the ICDAR 2013 dataset, this method had a good effect on foreground text erasure and background authenticity restoration. Experiments on a comprehensive dataset of real documents also showed good results. To achieve targeted removal of sensitive text information on pictures, we collected datasets based on real and synthetic documents, and experimental results were satisfactory. Compared to current and classic algorithms, our text removal algorithm performs best in Image Quality Assessment (IQA).},
author = {Zhangdao Huang and Jinglin Zhou},
journal = {2021 33rd Chinese Control and Decision Conference (CCDC)},
volume = {null},
pages = {1967-1972},
doi = {10.1109/CCDC52312.2021.9601654},
}

@article{00c8fb5e9be84801024842fb6be6814b361a4483,
title = {SwapText: Image Based Texts Transfer in Scenes},
year = {2020},
url = {https://www.semanticscholar.org/paper/00c8fb5e9be84801024842fb6be6814b361a4483},
abstract = {Swapping text in scene images while preserving original fonts, colors, sizes and background textures is a challenging task due to the complex interplay between different factors. In this work, we present SwapText, a three-stage framework to transfer texts across scene images. First, a novel text swapping network is proposed to replace text labels only in the foreground image. Second, a background completion network is learned to reconstruct background images. Finally, the generated foreground image and background image are used to generate the word image by the fusion network. Using the proposing framework, we can manipulate the texts of the input images even with severe geometric distortion. Qualitative and quantitative results are presented on several scene text datasets, including regular and irregular text datasets. We conducted extensive experiments to prove the usefulness of our method such as image based text translation, text image synthesis.},
author = {Qiangpeng Yang and Hongsheng Jin and Jun Huang and Wei Lin},
journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
volume = {null},
pages = {14688-14697},
doi = {10.1109/cvpr42600.2020.01471},
arxivid = {2003.08152},
}

@article{b8459104c93a493a598ebb21d5cf2cc8ff27bc5e,
title = {MTRNet: A Generic Scene Text Eraser},
year = {2019},
url = {https://www.semanticscholar.org/paper/b8459104c93a493a598ebb21d5cf2cc8ff27bc5e},
abstract = {Text removal algorithms have been proposed for uni-lingual scripts with regular shapes and layouts. However, to the best of our knowledge, a generic text removal method which is able to remove all or user-specified text regions regardless of font, script, language or shape is not available. Developing such a generic text eraser for real scenes is a challenging task, since it inherits all the challenges of multi-lingual and curved text detection and inpainting. To fill this gap, we propose a mask-based text removal network (MTRNet). MTRNet is a conditional adversarial generative network (cGAN) with an auxiliary mask. The introduced auxiliary mask not only makes the cGAN a generic text eraser, but also enables stable training and early convergence on a challenging large-scale synthetic dataset, initially proposed for text detection in real scenes. What's more, MTRNet achieves state-of-the-art results on several real-world datasets including ICDAR 2013, ICDAR 2017 MLT, and CTW1500, without being explicitly trained on this data, outperforming previous state-of-the-art methods trained directly on these datasets.},
author = {Osman Tursun and Rui Zeng and S. Denman and Sabesan Sivipalan and S. Sridharan and C. Fookes},
journal = {2019 International Conference on Document Analysis and Recognition (ICDAR)},
volume = {null},
pages = {39-44},
doi = {10.1109/ICDAR.2019.00016},
arxivid = {1903.04092},
}

@article{24f1a294737ab62e175a1574d337fa9e274464dc,
title = {PSSTRNet: Progressive Segmentation-Guided Scene Text Removal Network},
year = {2022},
url = {https://www.semanticscholar.org/paper/24f1a294737ab62e175a1574d337fa9e274464dc},
abstract = {Scene text removal (STR) is a challenging task due to the complex text fonts, colors, sizes, and background textures in scene images. However, most previous methods learn both text location and background inpainting implicitly within a single network, which weakens the text localization mecha-nism and makes a lossy background. To tackle these prob-lems, we propose a simple Progressive Segmentation-guided Scene Text Removal Network(PSSTRNet) to remove the text in the image iteratively. It contains two decoder branches, a text segmentation branch, and a text removal branch, with a shared encoder. The text segmentation branch generates text mask maps as the guidance for the regional removal branch. In each iteration, the original image, previous text removal result, and text mask are input to the network to extract the rest part of the text segments and cleaner text removal result. To get a more accurate text mask map, an update module is developed to merge the mask map in the current and previous stages. The final text removal result is obtained by adaptive fusion of results from all previous stages. A sufficient number of experiments and ablation studies conducted on the real and synthetic public datasets demonstrate our proposed method achieves state-of-the-art performance.},
author = {Guangtao Lyu and Anna Zhu},
journal = {2022 IEEE International Conference on Multimedia and Expo (ICME)},
volume = {null},
pages = {1-6},
doi = {10.1109/ICME52920.2022.9859792},
arxivid = {2306.07842},
}

@article{775113485783093d9cf246f4ef16fc40f61f20e4,
title = {Image Inpainting via Conditional Texture and Structure Dual Generation},
year = {2021},
url = {https://www.semanticscholar.org/paper/775113485783093d9cf246f4ef16fc40f61f20e4},
abstract = {Deep generative approaches have recently made considerable progress in image inpainting by introducing structure priors. Due to the lack of proper interaction with image texture during structure reconstruction, however, current solutions are incompetent in handling the cases with large corruptions, and they generally suffer from distorted results. In this paper, we propose a novel two-stream network for image inpainting, which models the structure-constrained texture synthesis and texture-guided structure reconstruction in a coupled manner so that they better leverage each other for more plausible generation. Furthermore, to enhance the global consistency, a Bi-directional Gated Feature Fusion (Bi-GFF) module is designed to exchange and combine the structure and texture information and a Contextual Feature Aggregation (CFA) module is developed to refine the generated contents by region affinity learning and multi-scale feature aggregation. Qualitative and quantitative experiments on the CelebA, Paris StreetView and Places2 datasets demonstrate the superiority of the proposed method. Our code is available at https://github.com/Xiefan-Guo/CTSDG.},
author = {Xiefan Guo and Hongyu Yang and Di Huang},
journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
volume = {null},
pages = {14114-14123},
doi = {10.1109/ICCV48922.2021.01387},
arxivid = {2108.09760},
}

@article{e326b6bbbe9412fbc64f321db5651b51282ec1ac,
title = {Mask-guided GAN for robust text editing in the scene},
year = {2021},
url = {https://www.semanticscholar.org/paper/e326b6bbbe9412fbc64f321db5651b51282ec1ac},
abstract = {S2 TL;DR: A mask-guided GAN method is proposed to adequately use the body, outline, and shadow of text to guide the task, decomposing the complex task into easy-to-learn ones.},
author = {Boxi Yu and Yong Xu and Yan Huang and Shuai Yang and Jiaying Liu},
journal = {Neurocomputing},
volume = {441},
pages = {192-201},
doi = {10.1016/J.NEUCOM.2021.02.045},
}

@article{1a5bcec6bbb8bb3abd4121543b759c56547c0a8c,
title = {Two-Stage Seamless Text Erasing On Real-World Scene Images},
year = {2021},
url = {https://www.semanticscholar.org/paper/1a5bcec6bbb8bb3abd4121543b759c56547c0a8c},
abstract = {Erasing text from images is a common image-editing task in film industry and shared media. Existing text-erasing models either tend to produce artifacts or fail to remove all the text in real-world images. In this paper, we follow a two-stage text erasing framework that first masks the text by segmentation, and then inpaints the masked region to create a text-erased image. Our proposed text mask generator is designed to accurately cover text, which combined with inpainting, can produce reliable text-erased results. In the inpainting model, we propose a Multiscale Gradient Reconstruction Loss to generate sharp realistic-looking images. Our model achieves state-of-the-art results on both synthetic and real world data in both quantitative and qualitative measures.},
author = {Benjamin Conrad and P. Chen},
journal = {2021 IEEE International Conference on Image Processing (ICIP)},
volume = {null},
pages = {1309-1313},
doi = {10.1109/ICIP42928.2021.9506394},
}

@article{7b39a42c7974e06063029ad48a4a1981fcafc845,
title = {Editing Text in the Wild},
year = {2019},
url = {https://www.semanticscholar.org/paper/7b39a42c7974e06063029ad48a4a1981fcafc845},
abstract = {In this paper, we are interested in editing text in natural images, which aims to replace or modify a word in the source image with another one while maintaining its realistic look. This task is challenging, as the styles of both background and text need to be preserved so that the edited image is visually indistinguishable from the source image. Specifically, we propose an end-to-end trainable style retention network (SRNet) that consists of three modules: text conversion module, background inpainting module and fusion module. The text conversion module changes the text content of the source image into the target text while keeping the original text style. The background inpainting module erases the original text, and fills the text region with appropriate texture. The fusion module combines the information from the two former modules, and generates the edited text images. To our knowledge, this work is the first attempt to edit text in natural images at the word level. Both visual effects and quantitative results on synthetic and real-world dataset (ICDAR 2013) fully confirm the importance and necessity of modular decomposition. We also conduct extensive experiments to validate the usefulness of our method in various real-world applications such as text image synthesis, augmented reality (AR) translation, information hiding, etc.},
author = {Liang Wu and Chengquan Zhang and Jiaming Liu and Junyu Han and Jingtuo Liu and Errui Ding and X. Bai},
journal = {Proceedings of the 27th ACM International Conference on Multimedia},
volume = {null},
pages = {null},
doi = {10.1145/3343031.3350929},
arxivid = {1908.03047},
}

@article{4959feace5f7ddcc6f4b55a2b0fbe41cfac6a3ad,
title = {Pyramid Mask Text Detector},
year = {2019},
url = {https://www.semanticscholar.org/paper/4959feace5f7ddcc6f4b55a2b0fbe41cfac6a3ad},
abstract = {Scene text detection, an essential step of scene text recognition system, is to locate text instances in natural scene images automatically. Some recent attempts benefiting from Mask R-CNN formulate scene text detection task as an instance segmentation problem and achieve remarkable performance. In this paper, we present a new Mask R-CNN based framework named Pyramid Mask Text Detector (PMTD) to handle the scene text detection. Instead of binary text mask generated by the existing Mask R-CNN based methods, our PMTD performs pixel-level regression under the guidance of location-aware supervision, yielding a more informative soft text mask for each text instance. As for the generation of text boxes, PMTD reinterprets the obtained 2D soft mask into 3D space and introduces a novel plane clustering algorithm to derive the optimal text box on the basis of 3D shape. Experiments on standard datasets demonstrate that the proposed PMTD brings consistent and noticeable gain and clearly outperforms state-of-the-art methods. Specifically, it achieves an F-measure of 80.13% on ICDAR 2017 MLT dataset.},
author = {Jingchao Liu and Xuebo Liu and Jie Sheng and Ding Liang and Xin Li and Qingjie Liu},
journal = {ArXiv},
volume = {abs/1903.11800},
pages = {null},
arxivid = {1903.11800},
}

@article{72468cc2983821ea423626323aea29c1f841f784,
title = {T-former: An Efficient Transformer for Image Inpainting},
year = {2022},
url = {https://www.semanticscholar.org/paper/72468cc2983821ea423626323aea29c1f841f784},
abstract = {Benefiting from powerful convolutional neural networks (CNNs), learning-based image inpainting methods have made significant breakthroughs over the years. However, some nature of CNNs (e.g. local prior, spatially shared parameters) limit the performance in the face of broken images with diverse and complex forms. Recently, a class of attention-based network architectures, called transformer, has shown significant performance on natural language processing fields and high-level vision tasks. Compared with CNNs, attention operators are better at long-range modeling and have dynamic weights, but their computational complexity is quadratic in spatial resolution, and thus less suitable for applications involving higher resolution images, such as image inpainting. In this paper, we design a novel attention linearly related to the resolution according to Taylor expansion. And based on this attention, a network called T-former is designed for image inpainting. Experiments on several benchmark datasets demonstrate that our proposed method achieves state-of-the-art accuracy while maintaining a relatively low number of parameters and computational complexity.},
author = {Ye Deng and Siqi Hui and Sanping Zhou and Deyu Meng and Jinjun Wang},
journal = {Proceedings of the 30th ACM International Conference on Multimedia},
volume = {null},
pages = {null},
doi = {10.1145/3503161.3548446},
arxivid = {2305.07239},
}

@article{a3c92c0033c46d59e62dc6144a7fea6ff55b8e57,
title = {PERT: A Progressively Region-based Network for Scene Text Removal},
year = {2021},
url = {https://www.semanticscholar.org/paper/a3c92c0033c46d59e62dc6144a7fea6ff55b8e57},
abstract = {Scene text removal (STR) contains two processes: text localization and background reconstruction. Through integrating both processes into a single network, previous methods provide an implicit erasure guidance by modifying all pixels in the entire image. However, there exists two problems: 1) the implicit erasure guidance causes the excessive erasure to non-text areas; 2) the one-stage erasure lacks the exhaustive removal of text region. In this paper, we propose a ProgrEssively Region-based scene Text eraser (PERT), introducing an explicit erasure guidance and performing balanced multi-stage erasure for accurate and exhaustive text removal. Firstly, we introduce a new region-based modification strategy (RegionMS) to explicitly guide the erasure process. Different from previous implicitly guided methods, RegionMS performs targeted and regional erasure on only text region, and adaptively perceives stroke-level information to improve the integrity of non-text areas with only bounding box level annotations. Secondly, PERT performs balanced multi-stage erasure with several progressive erasing stages. Each erasing stage takes an equal step toward the text-erased image to ensure the exhaustive erasure of text regions. Compared with previous methods, PERT outperforms them by a large margin without the need of adversarial loss, obtaining SOTA results with high speed (71 FPS) and at least 25% lower parameter complexity. Code is available at https://github.com/wangyuxin87/PERT.},
author = {Yuxin Wang and Hongtao Xie and Shancheng Fang and Yadong Qu and Yongdong Zhang},
arxivid = {2106.13029},
}

@article{88bd997588ebcbade1f05b9e823503ce61fa8c80,
title = {VCNet: A Robust Approach to Blind Image Inpainting},
year = {2020},
url = {https://www.semanticscholar.org/paper/88bd997588ebcbade1f05b9e823503ce61fa8c80},
abstract = {S2 TL;DR: This paper proposes a two-stage visual consistency network (VCN), meant to estimate where to fill (via masks) and generate what to fill and repairs these estimated missing regions using a new spatial normalization, enabling VCN to be robust to the mask prediction errors.},
author = {Yi Wang and Ying-Cong Chen and Xin Tao and Jiaya Jia},
journal = {ArXiv},
volume = {abs/2003.06816},
pages = {null},
doi = {10.1007/978-3-030-58595-2_45},
arxivid = {2003.06816},
}

@article{8c6dcb6e69acbd8d68a371e5a64d1ed1ce18c2c5,
title = {The Surprisingly Straightforward Scene Text Removal Method With Gated Attention and Region of Interest Generation: A Comprehensive Prominent Model Analysis},
year = {2022},
url = {https://www.semanticscholar.org/paper/2d3c2c81a3fb524cf824446b2a42c4e452f9404d},
abstract = {Scene text removal (STR), a task of erasing text from natural scene images, has recently attracted attention as an important component of editing text or concealing private information such as ID, telephone, and license plate numbers. While there are a variety of different methods for STR actively being researched, it is difficult to evaluate superiority because previously proposed methods do not use the same standardized training/evaluation dataset. We use the same standardized training/testing dataset to evaluate the performance of several previous methods after standardized re-implementation. We also introduce a simple yet extremely effective Gated Attention (GA) and Region-of-Interest Generation (RoIG) methodology in this paper. GA uses attention to focus on the text stroke as well as the textures and colors of the surrounding regions to remove text from the input image much more precisely. RoIG is applied to focus on only the region with text instead of the entire image to train the model more efficiently. Experimental results on the benchmark dataset show that our method significantly outperforms existing state-of-the-art methods in almost all metrics with remarkably higher-quality results. Furthermore, because our model does not generate a text stroke mask explicitly, there is no need for additional refinement steps or sub-models, making our model extremely fast with fewer parameters. The dataset and code are available at this https://github.com/naver/garnet.},
author = {Hyeonsu Lee and Chankyu Choi},
doi = {10.48550/arXiv.2210.07489},
arxivid = {2210.07489},
}

@article{d9b96b3e293cc38d28e5634659deff80c7d17edd,
title = {STELA: A Real-Time Scene Text Detector With Learned Anchor},
year = {2019},
url = {https://www.semanticscholar.org/paper/d9b96b3e293cc38d28e5634659deff80c7d17edd},
abstract = {To achieve high coverage of target boxes, a normal strategy of conventional one-stage anchor-based detectors is to utilize multiple priors at each spatial position, especially in scene text detection tasks. In this work, we present a simple and intuitive method for multi-oriented text detection where each location of feature maps only associates with one reference box. The idea is inspired from the two-stage R-CNN framework that can estimate the location of objects with any shape by using learned proposals. The aim of our method is to integrate this mechanism into a one-stage detector and employ the learned anchor which is obtained through a regression operation to replace the original one into the final predictions. Based on RetinaNet, our method achieves competitive performances on several public benchmarks with a totally real-time efficiency (<inline-formula> <tex-math notation="LaTeX">$26.5fps$ </tex-math></inline-formula> at <inline-formula> <tex-math notation="LaTeX">$800p$ </tex-math></inline-formula>), which surpasses all of anchor-based scene text detectors. In addition, with less attention on anchor design, we believe our method is easy to be applied on other analogous detection tasks. The code is publicly available at <uri>https://github.com/xhzdeng/stela</uri>.},
author = {Linjie Deng and Yanxiang Gong and Xinchen Lu and Yi Lin and Zheng Ma and M. Xie},
journal = {IEEE Access},
volume = {7},
pages = {153400-153407},
doi = {10.1109/ACCESS.2019.2948405},
arxivid = {1909.07549},
}

@article{350c619dacf870c45b67ed9253f1680beb491098,
title = {Text detection and removal from image using inpainting with smoothing},
year = {2015},
url = {https://www.semanticscholar.org/paper/350c619dacf870c45b67ed9253f1680beb491098},
abstract = {In this paper we have implemented a two stage frame work to remove unwanted text from images: first stage is to detect text from image and second stage is to remove that text using method of inpainting. To detect text, text localization and extraction is carried out followed by method of inpainting to fill the holes generated in image using surrounding region. To bring more efficiency in text detection smoothing works better. With the use of feature extraction, stroke filtering and centroid processing text detection is performed. Color histogram processing is carried out to carry clearer filtering of text components from other image components. In last stage, the text holes generated are filled with appropriate information present in same image using nearest matching neighborhood inpainting. To generate visually plausible region filling results, smoothing is carried out on the selected filled patches. We tested the implementation using different images, and compared the results of smoothing with results of implementation without smoothing. Experimental results show improved PSNR due to smoothing.},
author = {P. Wagh and D. Patil},
journal = {2015 International Conference on Pervasive Computing (ICPC)},
volume = {null},
pages = {1-4},
doi = {10.1109/PERVASIVE.2015.7087154},
}

@article{f4c1e311924a9b85f76b84dec7ddc73686af7acc,
title = {A generative image inpainting network based on the attention transfer network across layer mechanism},
year = {2021},
url = {https://www.semanticscholar.org/paper/f4c1e311924a9b85f76b84dec7ddc73686af7acc},
abstract = {S2 TL;DR: The attention transfer network cross layer proposed in this paper can effectively reconstruct the more detailed coding feature map, and it can play an active guiding role in the coding process by using skip connection; the image inpainting network can generate content that is highly consistent with the structure and semantics of the real image when repairing large-area irregular defect areas, which is more in line with the human visual experience.},
author = {Shi Yingnan and Fan Yao and Zhang Ningjun},
journal = {Optik},
volume = {242},
pages = {167101},
doi = {10.1016/J.IJLEO.2021.167101},
}

@article{b0b8fb294135dc738efdecc9f362f0745f9b28e1,
title = {Progressive Scene Text Erasing with Self-Supervision},
year = {2022},
url = {https://www.semanticscholar.org/paper/b65b8e4e5cd5f53c44fce0cdc661a5689f2b7d72},
abstract = {Scene text erasing seeks to erase text contents from scene images and current state-of-the-art text erasing models are trained on large-scale synthetic data. Although data synthetic engines can provide vast amounts of annotated training samples, there are differences between synthetic and real-world data. In this paper, we employ self-supervision for feature representation on unlabeled real-world scene text images. A novel pretext task is designed to keep consistent among text stroke masks of image variants. We design the Progressive Erasing Network in order to remove residual texts. The scene text is erased progressively by leveraging the intermediate generated results which provide the foundation for subsequent higher quality results. Experiments show that our method significantly improves the generalization of the text erasing task and achieves state-of-the-art performance on public benchmarks.},
author = {Xiangcheng Du and Zhao Zhou and Yingbin Zheng and Xingjiao Wu and Tianlong Ma and Cheng Jin},
journal = {Comput. Vis. Image Underst.},
volume = {233},
pages = {103712},
doi = {10.48550/arXiv.2207.11469},
arxivid = {2207.11469},
}

@article{6da6b6e0912385464064fd8cd34ffd7e5965e948,
title = {Image Inpainting With Learnable Edge-Attention Maps},
year = {2021},
url = {https://www.semanticscholar.org/paper/6da6b6e0912385464064fd8cd34ffd7e5965e948},
abstract = {This paper proposes an end-to-end Learnable Edge-Attention Map (LEAM) method to assist image inpainting. To achieve a better-recovered effect, we design an edge attention module, which extracts the feature information of the edge map and re-normalizes the image feature information when automatically updating the edge map. And the information of known regions is adopted to assist the decoder generates semantically consistent results. A dual-discriminator structure consisting of the local discriminator and global discriminator is proposed to generate realistic texture details and improve the consistency of the overall structure. Experiments show that our method can obtain higher image inpainting quality than the existing state-of-the-art approaches, which improves PSNR by 3.58%, SSIM by 2.27%, and reduce MAE by 9.21% on average.},
author = {Liujie Sun and Qinghan Zhang and Wenju Wang and Mingxi Zhang},
journal = {IEEE Access},
volume = {9},
pages = {3816-3827},
doi = {10.1109/ACCESS.2020.3047740},
}

@article{6066013d89e9776dda77a38f4c32b2ea7d9ecc06,
title = {Scene Text Eraser},
year = {2017},
url = {https://www.semanticscholar.org/paper/6066013d89e9776dda77a38f4c32b2ea7d9ecc06},
abstract = {The character information in natural scene images contains various personal information, such as telephone numbers, home addresses, etc. It is a high risk of leakage the information if they are published. In this paper, we proposed a scene text erasing method to properly hide the information via an inpainting convolutional neural network (CNN) model. The input is a scene text image, and the output is expected to be text erased image with all the character regions filled up the colors of the surrounding background pixels. This work is accomplished byaCNNmodelthroughconvolutiontodeconvolutionwithinterconnection process. The training samples and the corresponding inpainting images are considered as teaching signals for training. To evaluate the text erasing performance, the output images are detected by a novel scene text detection method. Subsequently, the same measurement on text detection is utilized for testing the images in benchmark dataset ICDAR2013. Compared with direct text detection way, the scene text erasing process demonstrates a drastically decrease on the precision, recall and f-score. That proves the effectiveness of proposed method for erasing the text in natural scene images.},
author = {Toshiki Nakamura and Anna Zhu and Keiji Yanai and S. Uchida},
journal = {2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
volume = {01},
pages = {832-837},
doi = {10.1109/ICDAR.2017.141},
arxivid = {1705.02772},
}

@article{b7fecd549152c7f7d7aa1364353cdbcc0e1ea3bc,
title = {Erasing Scene Text with Weak Supervision},
year = {2020},
url = {https://www.semanticscholar.org/paper/b7fecd549152c7f7d7aa1364353cdbcc0e1ea3bc},
abstract = {Scene text erasing is a task of removing text from natural scene images, which has been gaining attention in recent years. The main motivation is to conceal private information such as license plate numbers, and house nameplates that can appear in images. In this work, we propose a method for scene text erasing that approaches the problem as a general inpainting task. In contrast to previous methods, which require pairs of original images containing text and images from which the text has been removed, our method does not need corresponding image pairs for training. We use a separately trained scene text detector and an inpainting network. The scene text detector predicts segmentation maps of text instances which are then used as masks for the inpainting network. The network for inpainting, trained on a large-scale image dataset, fills in masked out regions in an input image and generates a final image in which the original text is no longer present. The results show that our method is able to successfully remove text and fill in the created holes to produce natural-looking images.},
author = {Jan Zdenek and Hideki Nakayama},
journal = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
volume = {null},
pages = {2227-2235},
doi = {10.1109/WACV45572.2020.9093544},
}

@article{68170ee9dec3c6108194b4f294d9c00119ad5de0,
title = {MTRNet++: One-stage Mask-based Scene Text Eraser},
year = {2019},
url = {https://www.semanticscholar.org/paper/68170ee9dec3c6108194b4f294d9c00119ad5de0},
abstract = {S2 TL;DR: The results of ablation studies demonstrate that the proposed multi-branch architecture with attention blocks is effective and essential, and demonstrates controllability and interpretability.},
author = {Osman Tursun and S. Denman and Rui Zeng and Sabesan Sivapalan and S. Sridharan and C. Fookes},
journal = {Comput. Vis. Image Underst.},
volume = {201},
pages = {103066},
doi = {10.1016/J.CVIU.2020.103066},
arxivid = {1912.07183},
}

@article{f018bba9b85e7a43e10ae48b1dd2890dbb801de3,
title = {Stroke-Based Scene Text Erasing Using Synthetic Data for Training},
year = {2021},
url = {https://www.semanticscholar.org/paper/f018bba9b85e7a43e10ae48b1dd2890dbb801de3},
abstract = {Scene text erasing, which replaces text regions with reasonable content in natural images, has drawn significant attention in the computer vision community in recent years. There are two potential subtasks in scene text erasing: text detection and image inpainting. Both subtasks require considerable data to achieve better performance; however, the lack of a large-scale real-world scene-text removal dataset does not allow existing methods to realize their potential. To compensate for the lack of pairwise real-world data, we made considerable use of synthetic text after additional enhancement and subsequently trained our model only on the dataset generated by the improved synthetic text engine. Our proposed network contains a stroke mask prediction module and background inpainting module that can extract the text stroke as a relatively small hole from the cropped text image to maintain more background content for better inpainting results. This model can partially erase text instances in a scene image with a bounding box or work with an existing scene-text detector for automatic scene text erasing. The experimental results from the qualitative and quantitative evaluation on the SCUT-Syn, ICDAR2013, and SCUT-EnsText datasets demonstrate that our method significantly outperforms existing state-of-the-art methods even when they are trained on real-world data.},
author = {Zhengmi Tang and Tomo Miyazaki and Yoshihiro Sugaya and S. Omachi},
journal = {IEEE Transactions on Image Processing},
volume = {30},
pages = {9306-9320},
doi = {10.1109/TIP.2021.3125260},
pmid = {34752394},
arxivid = {2104.11493},
}

@article{20334266b089240d6d723efdc17511d4929a434a,
title = {Recurrent Feature Reasoning for Image Inpainting},
year = {2020},
url = {https://www.semanticscholar.org/paper/20334266b089240d6d723efdc17511d4929a434a},
abstract = {Existing inpainting methods have achieved promising performance for recovering regular or small image defects. However, filling in large continuous holes remains difficult due to the lack of constraints for the hole center. In this paper, we devise a Recurrent Feature Reasoning (RFR) network which is mainly constructed by a plug-and-play Recurrent Feature Reasoning module and a Knowledge Consistent Attention (KCA) module. Analogous to how humans solve puzzles (i.e., first solve the easier parts and then use the results as additional information to solve difficult parts), the RFR module recurrently infers the hole boundaries of the convolutional feature maps and then uses them as clues for further inference. The module progressively strengthens the constraints for the hole center and the results become explicit. To capture information from distant places in the feature map for RFR, we further develop KCA and incorporate it in RFR. Empirically, we first compare the proposed RFR-Net with existing backbones, demonstrating that RFR-Net is more efficient (e.g., a 4% SSIM improvement for the same model size). We then place the network in the context of the current state-of-the-art, where it exhibits improved performance. The corresponding source code is available at: https://github.com/jingyuanli001/RFR-Inpainting},
author = {Jingyuan Li and N. Wang and Lefei Zhang and Bo Du and D. Tao},
journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
volume = {null},
pages = {7757-7765},
doi = {10.1109/cvpr42600.2020.00778},
arxivid = {2008.03737},
}

@article{0e434b198b65a74630c6387950ef9f828c212319,
title = {Automatic removal of various annotations and marking from maritime surveillance recorded video},
year = {2021},
url = {https://www.semanticscholar.org/paper/0e434b198b65a74630c6387950ef9f828c212319},
abstract = {One of the main challenging issues in video analysis is the recovery of the original video frames from the annotated and text marked in the guided user interface (GUI) tool, particularly in cases where the original video is not available. Removing annotation from video frames is essential for any kind of algorithm development, such as noise removal, dehazing, object detection, recognition, identification in the video and tracking specific objects in the maritime environment, and further testing process. In this research work, we developed the algorithm for the removal of all annotations from any portion of the video frame, without affecting the integrity of the original video content. Here, we present a novel technique to remove unnecessary annotations and markers using a progressive switching median filter with wavelet thresholding. Experimental studies have shown that the annotation-free images generated from the proposed method can be used for the development of any basic algorithm.},
author = {Issacniwas Swamidoss and Abdulla Al Mansoori and Abdulrahman Almarzooqi and Slim Sayadi},
doi = {10.1117/12.2599680},
}

@article{621b84717c355ed166f632c67f96d30970c58667,
title = {Scene text removal via cascaded text stroke detection and erasing},
year = {2021},
url = {https://www.semanticscholar.org/paper/621b84717c355ed166f632c67f96d30970c58667},
abstract = {S2 TL;DR: A novel end-to-end framework is proposed based on accurate text stroke detection that substantially outperforms the state-of-the-art for locating and erasing scene text.},
author = {Xuewei Bian and Chaoqun Wang and Weize Quan and Juntao Ye and Xiaopeng Zhang and Dong-Ming Yan},
journal = {Computational Visual Media},
volume = {8},
pages = {273 - 287},
doi = {10.1007/s41095-021-0242-8},
arxivid = {2011.09768},
}

@article{757cbefa26b5d23a159629f799400ef7f02620ea,
title = {FETNet: Feature erasing and transferring network for scene text removal},
year = {2023},
url = {https://www.semanticscholar.org/paper/757cbefa26b5d23a159629f799400ef7f02620ea},
abstract = {S2 TL;DR: A novel Feature Erasing and Transferring (FET) mechanism to reconfigure the encoded features for STR is proposed, and a one-stage, end-to-end trainable network called FETNet is constructed for scene text removal.},
author = {Guangtao Lyu and Kun Liu and Anna Zhu and S. Uchida and Brian Kenji Iwana},
journal = {ArXiv},
volume = {abs/2306.09593},
pages = {null},
doi = {10.1016/j.patcog.2023.109531},
arxivid = {2306.09593},
}

@article{efff0b0c84f8a3017fa7e10fddada41cbfda77fc,
title = {Don't Forget Me: Accurate Background Recovery for Text Removal via Modeling Local-Global Context},
year = {2022},
url = {https://www.semanticscholar.org/paper/efff0b0c84f8a3017fa7e10fddada41cbfda77fc},
abstract = {. Text removal has attracted increasingly attention due to its various applications on privacy protection, document restoration, and text editing. It has shown significant progress with deep neural network. However, most of the existing methods often generate inconsistent results for complex background. To address this issue, we propose a Contextual-guided Text Removal Network, termed as CTRNet. CTRNet explores both low-level structure and high-level discriminative context feature as prior knowledge to guide the process of text erasure and background restoration. We further propose a Local-global Content Modeling (LGCM) block with CNNs and Transformer-Encoder to capture local features and establish the long-term relationship among pixels globally. Finally, we incorporate LGCM with context guidance for feature modeling and decoding. Experiments on benchmark datasets, SCUT-EnsText and SCUT-Syn show that CTRNet significantly outperforms the existing state-of-the-art methods. Furthermore, a qualitative experiment on examination papers also demonstrates the generalizability of our method. The code of CTRNet is available at https://github.com/lcy0604/CTRNet.},
author = {Chongyu Liu and Lianwen Jin and Yuliang Liu and Canjie Luo and Bangdong Chen and Fengjun Guo and Kai Ding},
doi = {10.48550/arXiv.2207.10273},
arxivid = {2207.10273},
}

@article{57b7cecab32a960cc46cf9f232beca5b151a40b9,
title = {Image Inpainting With Learnable Bidirectional Attention Maps},
year = {2019},
url = {https://www.semanticscholar.org/paper/57b7cecab32a960cc46cf9f232beca5b151a40b9},
abstract = {Most convolutional network (CNN)-based inpainting methods adopt standard convolution to indistinguishably treat valid pixels and holes, making them limited in handling irregular holes and more likely to generate inpainting results with color discrepancy and blurriness. Partial convolution has been suggested to address this issue, but it adopts handcrafted feature re-normalization, and only considers forward mask-updating. In this paper, we present a learnable attention map module for learning feature re-normalization and mask-updating in an end-to-end manner, which is effective in adapting to irregular holes and propagation of convolution layers. Furthermore, learnable reverse attention maps are introduced to allow the decoder of U-Net to concentrate on filling in irregular holes instead of reconstructing both holes and known regions, resulting in our learnable bidirectional attention maps. Qualitative and quantitative experiments show that our method performs favorably against state-of-the-arts in generating sharper, more coherent and visually plausible inpainting results. The source code and pre-trained models will be available at: https://github.com/Vious/LBAM_inpainting/.},
author = {Chaohao Xie and Shaohui Liu and Chao Li and Ming-Ming Cheng and W. Zuo and Xiao Liu and Shilei Wen and Errui Ding},
journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
volume = {null},
pages = {8857-8866},
doi = {10.1109/ICCV.2019.00895},
arxivid = {1909.00968},
}

@article{1d7352732a2abb3ec8fb1e279d16896c26cf6304,
title = {Generative Image Inpainting for Large-Scale Edge Area},
year = {2021},
url = {https://www.semanticscholar.org/paper/1d7352732a2abb3ec8fb1e279d16896c26cf6304},
abstract = {In recent years, applying deep learning to computer vision is a very popular research direction, and a number of models with amazing effects have appeared. Deep learning-based approaches for end-to-end image inpainting have shown promise results. Recent research has made great progress in repairing rectangular and free-form areas but there are still many problems and room for improvement. For example, artifacts, blur and color missing still exist among the completion results of the large-scale border area. In this paper, we propose an end-to-end GAN-based image inpainting method, which has a better effect on the large boundary area. Our model is a two-stage adversarial network. The first stage completes the corresponding edge image, and the second stage uses the edge image generated in the first stage as a prior to complete the color image. We added parallel residual blocks to the edge completion network, and for the image completion network we replace the original residual blocks with multi-scale dilated convolution fusion blocks. Besides, a content loss based on DenseNet is added to the second stage. Experiments on multiple publicly available datasets show that our results have better effects on larger edge areas and can increase the average PSNR (Peak Signal to Noise Ratio) and SSIM (Structural Similarity Index).},
author = {Jiayi Liang and Xueming Li},
journal = {Proceedings of the 2021 3rd Asia Pacific Information Technology Conference},
volume = {null},
pages = {null},
doi = {10.1145/3449365.3449379},
}

@article{d20f64de2627528bd89fe06cf6207b7d2e21a367,
title = {Feature Correlational Attention Network for Image Inpainting},
year = {2022},
url = {https://www.semanticscholar.org/paper/d20f64de2627528bd89fe06cf6207b7d2e21a367},
abstract = {While existing methods based on generative model are able to restore missing contents with semantical coherency, the lack of combining the correlations between different levels results in the artifacts or blurry textures in the ultimate results. In this paper, we present a Feature Correlational Attention Network (FCAN) that leverages layers' correlations for better predictions. To be specific, we employ the U-Net as the basic framework to build our network, in which we implement each layer with the partial convolution method. Inside the correlational attention module, we choose the feature maps of the intermediate layer at both textural and structural levels, and perform a fusion block to emerge new correlational feature maps. An attention module is utilized to reconstruct generated patches with the kernels extracted from intermediate layers' features. Experiments on two benchmark datasets illustrate that our model generates promising results and outperforms the state-of-the-art methods.},
author = {Feihan Cao and Q. Zhu},
journal = {2022 IEEE 2nd International Conference on Data Science and Computer Application (ICDSCA)},
volume = {null},
pages = {994-1000},
doi = {10.1109/ICDSCA56264.2022.9988342},
}

@article{c64982e2761875c09cce51b107f322e3e6aeca0f,
title = {Deep learning-based image inpainting with structure map},
year = {2021},
url = {https://www.semanticscholar.org/paper/c64982e2761875c09cce51b107f322e3e6aeca0f},
abstract = {Abstract. In the past few years, deep learning-based image inpainting has made significant progress. However, many existing methods do not take into account the rationality of the structure and the fineness of the texture, which leads to the scattered structure or excessive smoothness of the repaired image. To solve this problem, we propose a two-stage image inpainting model composed of structure generation network and texture generation network. The structure generation network focuses on the structure and color domain and uses the damaged structure map extracted from the mask image to reasonably fill the mask area to generate a complete structure map. The texture generation network uses the repaired structure map to guide the refinement process. We train the two-stage network on the public datasets Places2, CelebA, and Paris StreetView, and the experimental results show the superiority of our method over the previous methods.},
author = {Dezhi Bo and Ran Ma and Keke Wang and Min Su and P. An},
journal = {Journal of Electronic Imaging},
volume = {30},
pages = {033028 - 033028},
doi = {10.1117/1.JEI.30.3.033028},
}

@article{fb7f380f98796525958f26113ecada1ef9d3cd3d,
title = {Generative image inpainting with residual texture prior and cross-layer contextual attention},
year = {2022},
url = {https://www.semanticscholar.org/paper/fb7f380f98796525958f26113ecada1ef9d3cd3d},
abstract = {Existing image inpainting methods have shown promising performance in filling the missing regions with visually plausible contents. However, these methods tend to produce distorted structure and blurry texture. To address these issues, in this paper we propose a two-stage inpainting network that combines texture generation and image completion. In the first stage, a texture generator is used to hallucinate texture of the missing regions to guide the reconstruction in the next stage. In the second stage, considering the texture prior would gradually lose its guiding role with the deepening of the network, we adopt residual texture prior to generate fine details. We also introduce a cross-layer contextual attention module which can not only learn contextual attention in decoder feature map, but also benefit from the similar feature shifted from the encoder, generating reasonable structure and realistic texture. Our comparison results of both qualitative analysis and quantitative experiments on Paris StreetView and CelebA datasets demonstrate our proposed method has better inpainting performance than existing methods.},
author = {Lei Li and Yuesheng Zhu},
doi = {10.1117/12.2645154},
}

@article{42c14218ab4d8eb435e31441e3738cec06362ce3,
title = {R-MNet: A Perceptual Adversarial Network for Image Inpainting},
year = {2020},
url = {https://www.semanticscholar.org/paper/42c14218ab4d8eb435e31441e3738cec06362ce3},
abstract = {Facial image inpainting is a problem that is widely studied, and in recent years the introduction of Generative Adversarial Networks, has led to improvements in the field. Unfortunately some issues persists, in particular when blending the missing pixels with the visible ones. We address the problem by proposing a Wasserstein GAN combined with a new reverse mask operator, namely Reverse Masking Network (R-MNet), a perceptual adversarial network for image inpainting. The reverse mask operator transfers the reverse masked image to the end of the encoder-decoder network leaving only valid pixels to be inpainted. Additionally, we propose a new loss function computed in feature space to target only valid pixels combined with adversarial training. These then capture data distributions and generate images similar to those in the training data with achieved realism (realistic and coherent) on the output images. We evaluate our method on publicly available dataset, and compare with state-of-the-art methods. We show that our method is able to generalize to high-resolution inpainting task, and further show more realistic outputs that are plausible to the human visual system when compared with the state-of-the-art methods. https://github.com/Jireh-Jam/R-MNet-Inpainting-keras},
author = {Jireh Jam and Connah Kendrick and Vincent Drouard and Kevin Walker and G. Hsu and Moi Hoon Yap},
journal = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
volume = {null},
pages = {2713-2722},
doi = {10.1109/WACV48630.2021.00276},
arxivid = {2008.04621},
}

@article{2a1ab46245530a5e662dee95980d7beac1777445,
title = {A Character Flow Framework for Multi-Oriented Scene Text Detection},
year = {2021},
url = {https://www.semanticscholar.org/paper/2a1ab46245530a5e662dee95980d7beac1777445},
abstract = {S2 TL;DR: A novel multi-oriented scene text detection framework, which includes two main modules: character instance segmentation and character flow construction, and a joint network of FPN and bidirectional long short-term memory (BLSTM) is developed to explore the context information among isolated characters, which are finally grouped into character flows.},
author = {Wenjun Yang and Beiji Zou and Kai-Wen Li and Shu Liu},
journal = {Journal of Computer Science and Technology},
volume = {36},
pages = {465 - 477},
doi = {10.1007/s11390-021-1362-4},
}

@article{00b3ed936071957322f260583c3ed4f86dfe4b17,
title = {Pixel‐wise Dense Detector for Image Inpainting},
year = {2020},
url = {https://www.semanticscholar.org/paper/00b3ed936071957322f260583c3ed4f86dfe4b17},
abstract = {Recent GAN‐based image inpainting approaches adopt an average strategy to discriminate the generated image and output a scalar, which inevitably lose the position information of visual artifacts. Moreover, the adversarial loss and reconstruction loss (e.g., ℓ1 loss) are combined with tradeoff weights, which are also difficult to tune. In this paper, we propose a novel detection‐based generative framework for image inpainting, which adopts the min‐max strategy in an adversarial process. The generator follows an encoder‐decoder architecture to fill the missing regions, and the detector using weakly supervised learning localizes the position of artifacts in a pixel‐wise manner. Such position information makes the generator pay attention to artifacts and further enhance them. More importantly, we explicitly insert the output of the detector into the reconstruction loss with a weighting criterion, which balances the weight of the adversarial loss and reconstruction loss automatically rather than manual operation. Experiments on multiple public datasets show the superior performance of the proposed framework. The source code is available at https://github.com/Evergrow/GDN_Inpainting.},
author = {Ruisong Zhang and Weize Quan and Baoyuan Wu and Zhifeng Li and Dong‐Ming Yan},
journal = {Computer Graphics Forum},
volume = {39},
pages = {null},
doi = {10.1111/cgf.14160},
arxivid = {2011.02293},
}

@article{d7a61eb225be56be3e3fc7f026213d237049d4d0,
title = {Image Inpainting with Edge-guided Learnable Bidirectional Attention Maps},
year = {2021},
url = {https://www.semanticscholar.org/paper/d7a61eb225be56be3e3fc7f026213d237049d4d0},
abstract = {For image inpainting, the convolutional neural networks (CNN) in previous methods often adopt standard convolutional operator, which treats valid pixels and holes indistinguishably. As a result, they are limited in handling irregular holes and tend to produce color-discrepant and blurry inpainting result. Partial convolution (PConv) copes with this issue by conducting masked convolution and feature re-normalization conditioned only on valid pixels, but the mask-updating is handcrafted and independent with image structural information. In this paper, we present an edge-guided learnable bidirectional attention map (Edge-LBAM) for improving image inpainting of irregular holes with several distinct merits. Instead of using a hard 0-1 mask, a learnable attention map module is introduced for learning feature re-normalization and mask-updating in an end-to-end manner. Learnable reverse attention maps are further proposed in the decoder for emphasizing on filling in unknown pixels instead of reconstructing all pixels. Motivated by that the filling-in order is crucial to inpainting results and largely depends on image structures in exemplar-based methods, we further suggest a multi-scale edge completion network to predict coherent edges. Our Edge-LBAM method contains dual procedures,including structure-aware mask-updating guided by predict edges and attention maps generated by masks for feature re-normalization.Extensive experiments show that our Edge-LBAM is effective in generating coherent image structures and preventing color discrepancy and blurriness, and performs favorably against the state-of-the-art methods in terms of qualitative metrics and visual quality.},
author = {Dongsheng Wang and Chaohao Xie and Shaohui Liu and Zhenxing Niu and W. Zuo},
journal = {ArXiv},
volume = {abs/2104.12087},
pages = {null},
arxivid = {2104.12087},
}
